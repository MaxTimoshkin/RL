{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjNFYFP3onJy"
      },
      "source": [
        "! pip install gym\n",
        "! git clone https://github.com/ClementRomac/gym-tictactoe\n",
        "% cd gym-tictactoe\n",
        "! python setup.py install\n",
        "\n",
        "import time\n",
        "import gym\n",
        "import gym_tictactoe\n",
        "import pickle\n",
        "import collections\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab983lABosEE"
      },
      "source": [
        "# Создает строку Q-таблицы для текущего состояния state, \n",
        "# Инициализируя все возможные action значениями по умолчанию Init_Q\n",
        "def q_row(state, init_q: float):\n",
        "    q = [0] * len(state)\n",
        "    for i in range(len(state)):\n",
        "        if (state[i] == 0):\n",
        "            q[i] = init_q\n",
        "        else: q[i] = -2   #Если клетка уже занята\n",
        "    return q\n",
        "\n",
        "# Определяет индекс максимального элемента массива\n",
        "def argmax(array):\n",
        "    max = array[0]\n",
        "    index = 0\n",
        "    for i in range(len(array)):\n",
        "        if (array[i] > max):\n",
        "            max = array[i]\n",
        "            index = i\n",
        "    return index\n",
        "\n",
        "# Хеширует state\n",
        "def get_hash(state):\n",
        "    result = 0\n",
        "    for k in range(len(state)):\n",
        "        result += (state[k] + 1) * (3 ** k)\n",
        "    return result\n",
        "\n",
        "# Оценка наград после окончания эпизода\n",
        "def update_reward(reward, user):\n",
        "    if done: \n",
        "        if reward == 10:        # Ничья\n",
        "            return 0\n",
        "\n",
        "        elif reward == 20:\n",
        "            if user == human: \n",
        "                return 1\n",
        "            elif user != human: \n",
        "                return -1\n",
        "    else: \n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUShsALjovHy"
      },
      "source": [
        "# Загрузка среды\n",
        "env = gym.make('TicTacToe-v1', symbols=[-1, 1], board_size=3, win_size=3)\n",
        "\n",
        "human = -1          # Обучающаяся сторона\n",
        "loop = True         # Эпизоды разыгрываются, пока loop = True\n",
        "alpha = 0.9         # Скорость обучения\n",
        "gamma = 0.95        # Коэффициент дисконтирования\n",
        "output = False      # Флаг, определяющий, будет ли происходить вывод в консоль\n",
        "q_table = {}        # Q-таблица - словарь {Hash(state): Q(action)}\n",
        "\n",
        "eps_greedy = 2      # Вероятность, с которой будет выбран случайный ход\n",
        "n_greedy = 1000     # Число эпизодов для изменения eps_greedy на h_greedy\n",
        "h_greedy = 0.1      # Шаг изменения вероятности\n",
        "flag_greedy = False # Флаг, устанавливающийся при выборе eps-greedy. \n",
        "                    # (Нужно для более точного критерия останова)\n",
        "\n",
        "n_stop = 1000       # Критерий останова \n",
        "                    # (Оптимальная политика не должна меняться n_stop раз)\n",
        "n_stop_cur = 0      # Количество раз без изменений политики\n",
        "episodes = 0        # Количество сыгранных эпизодов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EllH-IoKoyxK",
        "outputId": "7c9b45f7-c62b-4b5e-c266-8b2784e874a3"
      },
      "source": [
        "# Обучение\n",
        "%%time\n",
        "# Разыгрываются эпизоды\n",
        "while (loop):\n",
        "    user = -1       # Ходящий на данный момент игрок [-1: 'x', 1: 'o']\n",
        "\n",
        "    # Переменная наблюдения observation. Сброс настроек среды перед началом игры\n",
        "    if (user != human):\n",
        "        temp_state = env.reset() \n",
        "        old_hash = None     # Переменная для хранения старого хеша (состояния)\n",
        "    else:\n",
        "        state = env.reset()\n",
        "        old_hash = hash = get_hash(state)\n",
        "        q_table[hash] = q_row(state, 0.1)\n",
        "\n",
        "    done = False\n",
        "    reward = 0          # Награда\n",
        "\n",
        "    # Уменьшение eps_greedy каждые n_greedy эпизодов\n",
        "    if (episodes % n_greedy == 0):\n",
        "        eps_greedy -= h_greedy\n",
        "\n",
        "    while not done:\n",
        "        # Если ход не человека\n",
        "        if (user != human):\n",
        "            # Поиск случайной незанятой клетки\n",
        "            cell = env.action_space.sample() \n",
        "            while (temp_state[cell] != 0):\n",
        "                cell = env.action_space.sample()\n",
        "\n",
        "            # Анализ позиции после совершения хода  \n",
        "            state, reward, done, infos = env.step(cell, user)\n",
        "            if (output): env.render()\n",
        "\n",
        "            # Оценка наград\n",
        "            reward = update_reward(reward, user)\n",
        "\n",
        "            if not done:\n",
        "                # Если текущего state нет в Q-таблице, то нужно его добавить\n",
        "                hash = get_hash(state)\n",
        "                if (hash not in q_table):\n",
        "                    q_table[hash] = q_row(state, 0.1)\n",
        "\n",
        "                if (old_hash != None):\n",
        "                    y = reward + gamma * max(q_table[hash])\n",
        "                    q_table[old_hash][action] = (1 - alpha) * q_table[old_hash][action] + alpha * y\n",
        "\n",
        "                    if not flag_greedy:\n",
        "                        if (max(q_table[old_hash]) == q_table[old_hash][action]):\n",
        "                            n_stop_cur += 1\n",
        "                        else: n_stop_cur = 0\n",
        "                        if (n_stop_cur == n_stop):\n",
        "                            loop = False\n",
        "                old_hash = hash\n",
        "\n",
        "        #Если ход человека \n",
        "        if (user == human):\n",
        "            # Выбор хода по eps-greedy стратегии\n",
        "            if (random.random() <= eps_greedy):\n",
        "                action = env.action_space.sample() \n",
        "                while (state[action] != 0):\n",
        "                    action = env.action_space.sample()\n",
        "                flag_greedy = True\n",
        "            else:\n",
        "                action = argmax(q_table[hash])\n",
        "                flag_greedy = False\n",
        "\n",
        "            # Анализ позиции после совершения хода игроком\n",
        "            # temp_state - состояние для среды\n",
        "            temp_state, reward, done, infos = env.step(action, user)\n",
        "            reward = update_reward(reward, user)\n",
        "            if (output): env.render() \n",
        "\n",
        "        # Смена хода\n",
        "        if (user == 1): user = -1 \n",
        "        else: user = 1  \n",
        "\n",
        "    # Обновление последнего хода\n",
        "    q_table[hash][action] = reward\n",
        "\n",
        "    if (output): print('===============')\n",
        "    episodes += 1\n",
        "\n",
        "# Сохранение q-таблицы в файл\n",
        "with open('QLearn.pkl', 'wb') as f: \n",
        "    pickle.dump(q_table, f)\n",
        "print('episodes =', episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episodes = 21353\n",
            "CPU times: user 11.3 s, sys: 164 ms, total: 11.5 s\n",
            "Wall time: 15.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkZ6n96P1MZR"
      },
      "source": [
        "# Минимакс\n",
        "import numpy as np\n",
        "import pickle\n",
        "from collections import deque\n",
        "\n",
        "#Матрица выигрышей\n",
        "win_matrix = np.array([[1, 1, 1, 0, 0, 0, 0, 0, 0],     # Горизонтали\n",
        "                       [0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
        "                       [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
        "                       [1, 0, 0, 1, 0, 0, 1, 0, 0],     # Вертикали\n",
        "                       [0, 1, 0, 0, 1, 0, 0, 1, 0],\n",
        "                       [0, 0, 1, 0, 0, 1, 0, 0, 1],     \n",
        "                       [1, 0, 0, 0, 1, 0, 0, 0, 1],     # Диагонали\n",
        "                       [0, 0, 1, 0, 1, 0, 1, 0, 0]]).transpose()\n",
        "\n",
        "# Возвращает строку, в которой найдена победа\n",
        "def test_win(states, value):\n",
        "    multiply = np.dot(states, win_matrix)\n",
        "    i, j = np.where(multiply == value)\n",
        "    return i\n",
        "    \n",
        "#Вывод state на экран\n",
        "def print_state(state, symbols):\n",
        "    print('|-----------|')\n",
        "    for i in range(9):\n",
        "        if ((i % 3 == 0) & (i > 0)):\n",
        "            print('|\\n|-----------|')\n",
        "        if (state[i] == -1):\n",
        "            print('|', symbols[0], end=' ')\n",
        "        elif (state[i] == 1):\n",
        "            print('|', symbols[1], end=' ')\n",
        "        else:\n",
        "            print('|  ', end=' ')\n",
        "    print('|\\n|-----------|')\n",
        "\n",
        "# Хеширует state\n",
        "def get_hash(state):\n",
        "    result = 0\n",
        "    for k in range(np.size(state)):\n",
        "        result += (state[k] + 1) * (3 ** k)\n",
        "    return result\n",
        "\n",
        "# Создает q-строку, инициазизируя все нулем и учитывая занятые клетки (-2)\n",
        "def state_to_q(state, is_busy):\n",
        "    q_row = np.zeros(np.size(state))\n",
        "    for i in range(np.size(state)):\n",
        "        if (state[i] != 0):\n",
        "            q_row[i] = is_busy # Клетка занята\n",
        "        else:\n",
        "            q_row[i] = 0\n",
        "    return q_row\n",
        "\n",
        "# Оценка. Ход наш\n",
        "def mark(state, n_move):\n",
        "    # Создание q-строки\n",
        "    hash = get_hash(state)\n",
        "    q[hash] = state_to_q(state, -2)\n",
        "\n",
        "    # Очередь под действия\n",
        "    actions = deque()\n",
        "\n",
        "    #Матрица возможных состояний\n",
        "    new_states = np.zeros((size - n_move, size))\n",
        "    new_states[:] = np.copy(state)\n",
        "    i = 0\n",
        "    for j in range (size):\n",
        "        if (state[j] == 0):\n",
        "            new_states[i][j] = -1\n",
        "            i += 1\n",
        "            actions.append(j)\n",
        "\n",
        "    # Поиск выигрышей\n",
        "    index_win = test_win(new_states, -3)\n",
        "    if (np.size(index_win)):\n",
        "        for k in index_win:\n",
        "            q[hash][actions[k]] = 1 #Победа\n",
        "        return 1\n",
        "    else:\n",
        "        max = -1\n",
        "        for k in range(len(new_states)):\n",
        "            q[hash][actions[k]] = mark2(new_states[k], n_move + 1)\n",
        "            if (q[hash][actions[k]] == 1):\n",
        "                return 1\n",
        "            if (q[hash][actions[k]] > max):\n",
        "                max = q[hash][actions[k]]\n",
        "        return max\n",
        "\n",
        "# Оценка. Ход врага\n",
        "def mark2(state, n_move):\n",
        "    if (n_move == 9):   #Ничья\n",
        "        return 0\n",
        "\n",
        "    # Создание q-строки\n",
        "    hash = get_hash(state)\n",
        "    q[hash] = state_to_q(state, 2)\n",
        "\n",
        "    # Очередь под действия\n",
        "    actions = deque()\n",
        "\n",
        "    #Матрица возможных состояний\n",
        "    new_states = np.zeros((size - n_move, size))\n",
        "    new_states[:] = np.copy(state)\n",
        "    i = 0\n",
        "    for j in range (size):\n",
        "        if (state[j] == 0):\n",
        "            new_states[i][j] = 1\n",
        "            i += 1\n",
        "            actions.append(j)\n",
        "\n",
        "    # Поиск проигрышей\n",
        "    index_win = test_win(new_states, 3)\n",
        "    if (np.size(index_win)):\n",
        "        for k in index_win:\n",
        "            q[hash][actions[k]] = -1\n",
        "        return -1 # Поражение\n",
        "    else:\n",
        "        min = 1\n",
        "        for k in range(len(new_states)):\n",
        "            q[hash][actions[k]] = mark(new_states[k], n_move + 1)\n",
        "            if (q[hash][actions[k]] == -1):\n",
        "                return -1\n",
        "            if (q[hash][actions[k]] < min):\n",
        "                min = q[hash][actions[k]]\n",
        "        return min\n",
        "\n",
        "size = 9                # Размер игрового поля\n",
        "state = np.zeros(size)\n",
        "\n",
        "user = -1\n",
        "n_move = 0              # Количество поставленных фигур (Помогает ловить ничью)\n",
        "\n",
        "q = {}\n",
        "\n",
        "mark(state, 0)\n",
        "\n",
        "# Сохранение q-таблицы в файл\n",
        "with open('MinMax_2.pkl', 'wb') as f:\n",
        "    pickle.dump(q, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG49mRuSpL3r",
        "outputId": "d5fed373-764c-4b08-8eb5-f60d59f9ba13"
      },
      "source": [
        "#Q-table VS MinMax\n",
        "\n",
        "env = gym.make('TicTacToe-v1', symbols=[-1, 1], board_size=3, win_size=3)\n",
        "human = -1           # Обучающаяся сторона\n",
        "reward = 0          # Награда\n",
        "episodes = 5000\n",
        "output = False\n",
        "output_result = False\n",
        "min_max_wins = 0\n",
        "q_learn_wins = 0\n",
        "\n",
        "# Загрузка q-таблицы из файла\n",
        "with open('QLearn.pkl', 'rb') as f: \n",
        "    q_table = pickle.load(f)\n",
        "\n",
        "with open('MinMax_2.pkl', 'rb') as f: \n",
        "    min_max = pickle.load(f)\n",
        "\n",
        "#Обученный ПК vs Человек\n",
        "for i in range(episodes):\n",
        "    state = env.reset() # Переменная наблюдения observation. Сброс настроек среды перед началом игры\n",
        "    user = -1           # Ходящий на данный момент игрок [-1: 'x', 1: 'o']\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Ход MinMax\n",
        "        if (user != human):   \n",
        "            hash = get_hash(state)\n",
        "            cell = random.choice((np.where(min_max[hash] == np.max(min_max[hash])))[0])\n",
        "            if (output): env.render() \n",
        "        \n",
        "        # Ход Q-table\n",
        "        if (user == human):\n",
        "            hash = get_hash(state)\n",
        "            if (hash not in q_table):\n",
        "                q_table[hash] = q_row(state, 0.6)\n",
        "            cell = argmax(q_table[hash])\n",
        "            if (output): env.render() \n",
        "        \n",
        "        # Анализ позиции после совершения хода игроком  \n",
        "        state, reward, done, infos = env.step(cell, user)       \n",
        "\n",
        "        # Оценка наград после окончания эпизода\n",
        "        if done:                                            \n",
        "            if reward == 10:\n",
        "                reward = 0.5\n",
        "                if (output | output_result): print('Ничья')\n",
        "\n",
        "            elif reward == 20:\n",
        "                if user == human: \n",
        "                    if (output | output_result): print('QLearn выиграл')\n",
        "                    q_learn_wins += 1\n",
        "                elif user != human: \n",
        "                    if (output | output_result): print('MinMax выиграл')\n",
        "                    min_max_wins += 1\n",
        "\n",
        "        # Смена хода\n",
        "        if (user == 1): user = -1 \n",
        "        else: user = 1  \n",
        "\n",
        "    # Вывод поля в консоль после окончания эпизода\n",
        "    if (output | output_result): env.render()\n",
        "    if (output | output_result): print('===============')\n",
        "\n",
        "print('num_episodes = ', episodes)\n",
        "print('minimax wins = ', min_max_wins)\n",
        "print('q-learn wins = ', q_learn_wins)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_episodes =  5000\n",
            "minimax wins =  0\n",
            "q-learn wins =  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPx-2ehaqMr8"
      },
      "source": [
        "# Игра с ообученным ПК\n",
        "# Загрузка q-таблицы из файла\n",
        "with open('QLearn.pkl', 'rb') as f: \n",
        "    q_table = pickle.load(f)\n",
        "\n",
        "#Обученный ПК vs Человек\n",
        "for i in range(5):\n",
        "    state = env.reset() # Переменная наблюдения observation. Сброс настроек среды перед началом игры\n",
        "    user = -1           # Ходящий на данный момент игрок [-1: 'x', 1: 'o']\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Если ходит человек\n",
        "        if (user != human):   \n",
        "            env.render() \n",
        "            print(\"Введите ваш ход (Строка, столбец):\")\n",
        "            row, col = map(int, input().split())    # Ввод строки и столбца\n",
        "            cell = (row - 1) * int(len(state) ** 0.5) + col - 1 # Клетка, выбранная человеком\n",
        "            while (state[cell] != 0):\n",
        "                print(\"Клетка занята!\")\n",
        "                row, col = map(int, input().split())   # Ввод строки и столбца\n",
        "                cell = (row - 1) * int(len(state) ** 0.5) + col - 1 # Клетка, выбранная человеком\n",
        "        \n",
        "        # Если ход ПК и текущего state нет в Q-таблице, то нужно его добавить\n",
        "        if (user == human):\n",
        "            hash = get_hash(state)\n",
        "            if (hash not in q_table):\n",
        "                q_table[hash] = q_row(state, 0.6)\n",
        "            cell = argmax(q_table[hash])\n",
        "            env.render() \n",
        "        \n",
        "        # Анализ позиции после совершения хода игроком  \n",
        "        state, reward, done, infos = env.step(cell, user)       \n",
        "\n",
        "        # Оценка наград после окончания эпизода\n",
        "        if done:                                            \n",
        "            if reward == 10:\n",
        "                print('Ничья')\n",
        "\n",
        "            elif reward == 20:\n",
        "                if user == human: \n",
        "                    print('Вы проиграли')\n",
        "                elif user != human: \n",
        "                    print('Вы выиграли')\n",
        "\n",
        "        # Смена хода\n",
        "        if (user == 1): user = -1 \n",
        "        else: user = 1  \n",
        "\n",
        "    # Вывод поля в консоль после окончания эпизода\n",
        "    env.render()\n",
        "    print('===============')\n",
        "\n",
        "# Сохранение q-таблицы в файл\n",
        "with open('QLearn.pkl', 'wb') as f:\n",
        "    pickle.dump(q_table, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFsUa1NZxj1Y"
      },
      "source": [
        "# Обучение, рассчитывающее в процессе процент побед каждые n партий\n",
        "\n",
        "# Оценка наград после окончания эпизода\n",
        "def update_reward(reward, user):\n",
        "    if done: \n",
        "        if reward == 10:        # Ничья\n",
        "            return 0\n",
        "\n",
        "        elif reward == 20:\n",
        "            if user == human:\n",
        "                return 1\n",
        "\n",
        "            elif user != human:\n",
        "                return -1\n",
        "    else: \n",
        "        return 0\n",
        "\n",
        "cur_wins = 0          # Текущее количество побед\n",
        "cur_loss = 0          # Текущее количество поражений\n",
        "max_episodes = 30000  # Максимальное число эпизодов\n",
        "percent_wins = {}     # Массив для хранения процента выигрышей каждые n_ep эпизодов\n",
        "percent_loss = {}     # Массив для хранения процента проигрышей каждые n_ep эпизодов\n",
        "n_ep = 1000\n",
        "episodes = 0\n",
        "\n",
        "# Разыгрываются эпизоды\n",
        "while (episodes <= max_episodes):\n",
        "    user = -1       # Ходящий на данный момент игрок [-1: 'x', 1: 'o']\n",
        "\n",
        "    # Переменная наблюдения observation. Сброс настроек среды перед началом игры\n",
        "    if (user != human):\n",
        "        temp_state = env.reset() \n",
        "        old_hash = None     # Переменная для хранения старого хеша (состояния)\n",
        "    else:\n",
        "        state = env.reset()\n",
        "        old_hash = hash = get_hash(state)\n",
        "        q_table[hash] = q_row(state, -0.5)\n",
        "\n",
        "    done = False\n",
        "    reward = 0          # Награда\n",
        "\n",
        "    # Уменьшение eps_greedy каждые n_greedy эпизодов\n",
        "    if (episodes % n_greedy == 0):\n",
        "        eps_greedy -= h_greedy\n",
        "\n",
        "    if (episodes % n_ep == n_ep - 1):\n",
        "        percent_wins[(int)(episodes / n_ep) + 1] = cur_wins / n_ep * 100\n",
        "        percent_loss[(int)(episodes / n_ep) + 1] = cur_loss / n_ep * 100\n",
        "        cur_wins = 0\n",
        "        cur_loss = 0\n",
        "\n",
        "    while not done:\n",
        "        # Если ход не человека\n",
        "        if (user != human):\n",
        "            # Поиск случайной незанятой клетки\n",
        "            cell = env.action_space.sample() \n",
        "            while (temp_state[cell] != 0):\n",
        "                cell = env.action_space.sample()\n",
        "\n",
        "            # Анализ позиции после совершения хода  \n",
        "            state, reward, done, infos = env.step(cell, user)\n",
        "            if (output): env.render()\n",
        "\n",
        "            # Оценка наград\n",
        "            reward = update_reward(reward, user)\n",
        "            if (reward > 0): cur_wins += 1\n",
        "            if (reward < 0): cur_loss += 1\n",
        "\n",
        "            if not done:\n",
        "                # Если текущего state нет в Q-таблице, то нужно его добавить\n",
        "                hash = get_hash(state)\n",
        "                if (hash not in q_table):\n",
        "                    q_table[hash] = q_row(state, 0.1)\n",
        "\n",
        "                if (old_hash != None):\n",
        "                    y = reward + gamma * max(q_table[hash])\n",
        "                    q_table[old_hash][action] = (1 - alpha) * q_table[old_hash][action] + alpha * y\n",
        "\n",
        "                    if not flag_greedy:\n",
        "                        if (max(q_table[old_hash]) == q_table[old_hash][action]):\n",
        "                            n_stop_cur += 1\n",
        "                        else: n_stop_cur = 0\n",
        "                        if (n_stop_cur == n_stop):\n",
        "                            loop = False\n",
        "                old_hash = hash\n",
        "\n",
        "        #Если ход человека \n",
        "        if (user == human):\n",
        "            # Выбор хода по eps-greedy стратегии\n",
        "            if (random.random() <= eps_greedy):\n",
        "                action = env.action_space.sample() \n",
        "                while (state[action] != 0):\n",
        "                    action = env.action_space.sample()\n",
        "                flag_greedy = True\n",
        "            else:\n",
        "                action = argmax(q_table[hash])\n",
        "                flag_greedy = False\n",
        "\n",
        "            # Анализ позиции после совершения хода игроком\n",
        "            # temp_state - состояние для среды\n",
        "            temp_state, reward, done, infos = env.step(action, user)\n",
        "            reward = update_reward(reward, user)\n",
        "            if (reward > 0): cur_wins += 1\n",
        "            if (reward < 0): cur_loss += 1\n",
        "            if (output): env.render() \n",
        "\n",
        "        # Смена хода\n",
        "        if (user == 1): user = -1 \n",
        "        else: user = 1  \n",
        "\n",
        "    # Обновление последнего хода\n",
        "    q_table[hash][action] = reward\n",
        "\n",
        "    if (output): print('===============')\n",
        "    episodes += 1\n",
        "\n",
        "# Сохранение q-таблицы в файл\n",
        "with open('QLearn.pkl', 'wb') as f: \n",
        "    pickle.dump(q_table, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "GYRDefDs5dEy",
        "outputId": "741fe75e-1dba-46d5-85e3-204128de9d65"
      },
      "source": [
        "# Построение графиков\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame({'Число эпизодов': pd.Series(list(percent_wins.keys())) * n_ep, 'Число проигрышей': percent_loss, 'Число выигрышей': percent_wins}) \n",
        "plt.plot(df['Число эпизодов'], df['Число выигрышей'])\n",
        "plt.plot(df['Число эпизодов'], df['Число проигрышей'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7649654810>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiTNubqO1NqV"
      },
      "source": [
        "# Игра против минимакса\n",
        "\n",
        "# Определяет индекс минимального элемента массива\n",
        "def argmin(array):\n",
        "    min = 2\n",
        "    index = 0\n",
        "    for i in range(len(array)):\n",
        "        if (array[i] != -2) & (array[i] < min):\n",
        "            min = array[i]\n",
        "            index = i\n",
        "    return index\n",
        "\n",
        "\n",
        "# Загрузка q-таблицы из файла\n",
        "human = -1\n",
        "\n",
        "with open('MinMax_2.pkl', 'rb') as f: \n",
        "    q_table = pickle.load(f)\n",
        "\n",
        "#Обученный ПК vs Человек\n",
        "for i in range(5):\n",
        "    state = env.reset() # Переменная наблюдения observation. Сброс настроек среды перед началом игры\n",
        "    user = -1           # Ходящий на данный момент игрок [-1: 'x', 1: 'o']\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Если ходит человек\n",
        "        if (user == human):   \n",
        "            env.render() \n",
        "            print(\"Введите ваш ход (Строка, столбец):\")\n",
        "            row, col = map(int, input().split())    # Ввод строки и столбца\n",
        "            cell = (row - 1) * int(len(state) ** 0.5) + col - 1 # Клетка, выбранная человеком\n",
        "            while (state[cell] != 0):\n",
        "                print(\"Клетка занята!\")\n",
        "                row, col = map(int, input().split())   # Ввод строки и столбца\n",
        "                cell = (row - 1) * int(len(state) ** 0.5) + col - 1 # Клетка, выбранная человеком\n",
        "        \n",
        "        # Если ход ПК (minmax)\n",
        "        if (user != human):\n",
        "            hash = get_hash(state)\n",
        "            if (human ==  1): cell = argmax(q_table[hash])\n",
        "            if (human == -1): cell = argmin(q_table[hash])\n",
        "            print(cell)\n",
        "            env.render() \n",
        "        \n",
        "        # Анализ позиции после совершения хода игроком  \n",
        "        state, reward, done, infos = env.step(cell, user)       \n",
        "\n",
        "        # Оценка наград после окончания эпизода\n",
        "        if done:                                            \n",
        "            if reward == 10:\n",
        "                print('Ничья')\n",
        "\n",
        "            elif reward == 20:\n",
        "                if user == human: \n",
        "                    print('Вы проиграли')\n",
        "                elif user != human: \n",
        "                    print('Вы выиграли')\n",
        "\n",
        "        # Смена хода\n",
        "        if (user == 1): user = -1 \n",
        "        else: user = 1  \n",
        "\n",
        "    # Вывод поля в консоль после окончания эпизода\n",
        "    env.render()\n",
        "    print('===============')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}